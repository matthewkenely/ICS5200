{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\n",
    "    'angry',\n",
    "    'disgust',\n",
    "    'fear',\n",
    "    'happy',\n",
    "    'neutral',\n",
    "    'sad',\n",
    "    'surprise'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, root_dir, emotions, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i, emotion in enumerate(emotions):\n",
    "            emotion_dir = os.path.join(root_dir, emotion)\n",
    "            if os.path.isdir(emotion_dir):  # Ensure it's a directory\n",
    "                for image_name in os.listdir(emotion_dir):\n",
    "                    image_path = os.path.join(emotion_dir, image_name)\n",
    "                    self.images.append(image_path)\n",
    "                    self.labels.append(i)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        image = Image.open(image_path).convert('L')  # Open as grayscale\n",
    "        # image = Image.open(image_path).convert('RGB')  # Open as RGB\n",
    "        # Convert to tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.resnet = models.resnet101(pretrained=True)\n",
    "\n",
    "        # Modify the ResNet model to accept grayscale images\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # Remove the final fully connected layer\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the last layer\n",
    "\n",
    "        # Freeze all layers of ResNet\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last few layers for fine-tuning\n",
    "        for layer in list(self.resnet.children())[:]:  # Get the last two layers\n",
    "            for param in layer.parameters():  # Access parameters of the layer\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Add additional layers: 2 fully connected layers and an output layer\n",
    "        self.fc1 = nn.Linear(2048, 512)  # 2048 is the output of the last ResNet layer\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Batch normalization for the first fully connected layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)  # Batch normalization for the output layer\n",
    "\n",
    "        # Optional: Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # LogSoftmax layer\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the ResNet backbone\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output from ResNet\n",
    "\n",
    "        # Forward pass through the additional fully connected layers with batch normalization\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # Apply LogSoftmax to get log probabilities\n",
    "        x = self.log_softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.002\n",
    "NUM_CLASSES = 7\n",
    "ROOT_DIR = './images'  # Adjust based on your dataset structure\n",
    "\n",
    "# Define data transformations\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((48, 48)),  # Resize to the target dimensions\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip horizontally\n",
    "    # transforms.RandomRotation(degrees=15),  # Rotate randomly within 15 degrees\n",
    "    # transforms.RandomResizedCrop(size=48, scale=(0.8, 1.0)),  # Random crop and resize\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Randomly change brightness, contrast, saturation, hue\n",
    "    # transforms.RandomVerticalFlip(p=0.5),  # 50% chance to flip vertically (optional)\n",
    "    transforms.ToTensor(),  # Convert to tensor before applying RandomErasing\n",
    "    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.33)),  # Randomly erase parts of the image\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalization for grayscale\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])\n",
    "\n",
    "# Initialize datasets and dataloaders\n",
    "train_dataset = EmotionDataset(root_dir=os.path.join(ROOT_DIR, 'train'), emotions=emotions, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = EmotionDataset(root_dir=os.path.join(ROOT_DIR, 'validation'), emotions=emotions, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = {\n",
    "    0: 3993,  # angry\n",
    "    1: 436,   # disgust\n",
    "    2: 4103,  # fear\n",
    "    3: 7164,  # happy\n",
    "    4: 4982,  # neutral\n",
    "    5: 4938,  # sad\n",
    "    6: 3205   # surprise\n",
    "}\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "# Calculate class weights as the inverse of the frequency\n",
    "class_weights = {label: total_samples / (num_classes * count) for label, count in class_counts.items()}\n",
    "class_weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Training Epoch 1/50: 100%|██████████| 226/226 [00:35<00:00,  6.31batch/s, loss=1.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 17.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 226/226 [00:35<00:00,  6.39batch/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Loss: 1.8126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 28.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 226/226 [00:35<00:00,  6.38batch/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Loss: 1.6505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 38.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 226/226 [00:35<00:00,  6.37batch/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Loss: 1.5461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 42.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 226/226 [00:35<00:00,  6.38batch/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Loss: 1.4698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 42.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Loss: 1.3711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Loss: 1.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Loss: 1.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 47.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Loss: 1.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 48.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/50: 100%|██████████| 226/226 [00:35<00:00,  6.38batch/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 1.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/50: 100%|██████████| 226/226 [00:35<00:00,  6.37batch/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Loss: 1.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Loss: 1.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/50: 100%|██████████| 226/226 [00:35<00:00,  6.35batch/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Loss: 1.1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/50: 100%|██████████| 226/226 [00:35<00:00,  6.37batch/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Loss: 1.1495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15/50: 100%|██████████| 226/226 [00:35<00:00,  6.35batch/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Loss: 1.1394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16/50: 100%|██████████| 226/226 [00:35<00:00,  6.35batch/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Loss: 1.1432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17/50: 100%|██████████| 226/226 [00:35<00:00,  6.33batch/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Loss: 1.1424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18/50: 100%|██████████| 226/226 [00:35<00:00,  6.36batch/s, loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/50], Loss: 1.1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19/50: 100%|██████████| 226/226 [00:35<00:00,  6.35batch/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/50], Loss: 1.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20/50: 100%|██████████| 226/226 [00:35<00:00,  6.34batch/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/50], Loss: 1.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 12.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21/50: 100%|██████████| 226/226 [00:35<00:00,  6.33batch/s, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/50], Loss: 1.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 56/56 [00:04<00:00, 11.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 51.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22/50:  20%|█▉        | 45/226 [00:07<00:28,  6.32batch/s, loss=1.13]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Use tqdm to create a progress bar for training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Zero the gradients\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mEmotionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py:1276\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1276\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/functional.py:898\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    896\u001b[0m     _log_api_usage_once(adjust_contrast)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:82\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m enhancer \u001b[38;5;241m=\u001b[39m \u001b[43mImageEnhance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m img \u001b[38;5;241m=\u001b[39m enhancer\u001b[38;5;241m.\u001b[39menhance(contrast_factor)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/PIL/ImageEnhance.py:75\u001b[0m, in \u001b[0;36mContrast.__init__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mImageStat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegenerate \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m, image\u001b[38;5;241m.\u001b[39msize, mean)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegenerate\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mmode:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/PIL/ImageStat.py:123\u001b[0m, in \u001b[0;36mStat.mean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Average (arithmetic mean) pixel level for each band in the image.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbands\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/PIL/ImageStat.py:123\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Average (arithmetic mean) pixel level for each band in the image.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m[i] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbands]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/functools.py:999\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;66;03m# check if another thread filled cache while we awaited lock\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m         val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m   1001\u001b[0m             val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = EmotionClassifier(num_classes=NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "# Check if GPU is available\n",
    "# device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Use tqdm to create a progress bar for training\n",
    "    with tqdm(total=len(train_loader), desc=f'Training Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix(loss=running_loss / (pbar.n + 1))  # Update loss in the progress bar\n",
    "            pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Use tqdm to create a progress bar for validation\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc='Validation', unit='batch') as pbar:\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                pbar.update(1)  # Increment the progress bar\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    scheduler.step(accuracy)\n",
    "    \n",
    "    print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print('Training Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6941,  0.6706,  0.6627,  ..., -0.7569, -0.6392, -0.2863],\n",
       "          [ 0.6941,  0.7176,  0.6706,  ..., -0.8118, -0.7882, -0.4667],\n",
       "          [ 0.6706,  0.7098,  0.7412,  ..., -0.8667, -0.8118, -0.4275],\n",
       "          ...,\n",
       "          [ 0.8824,  0.9216,  0.9216,  ...,  0.4353,  0.4039,  0.4275],\n",
       "          [ 0.8745,  0.8824,  0.9216,  ...,  0.4196,  0.4118,  0.4588],\n",
       "          [ 0.9294,  0.8667,  0.8980,  ...,  0.4745,  0.4745,  0.4824]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make new predictions\n",
    "model.eval()\n",
    "test_dataset = EmotionDataset(root_dir=os.path.join(ROOT_DIR, 'validation'), emotions=emotions, transform=transform)\n",
    "\n",
    "r = randint(0, len(test_dataset) - 1)\n",
    "\n",
    "# Select a random image from the dataset\n",
    "image, label = test_dataset[r]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "output = model(image)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image.cpu().squeeze().numpy(), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(f'Predicted: {emotions[predicted.item()]}, Actual: {emotions[label]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
